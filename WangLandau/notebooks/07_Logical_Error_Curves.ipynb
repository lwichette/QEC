{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mpmath as mp\n",
    "mp.mp.dps = 50 #higher precision\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.integrate as integrate\n",
    "from plotDensity import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_wrt_e(walker_results, precision=10):\n",
    "    derivatives = []\n",
    "    for result in walker_results:\n",
    "        derivative = {}\n",
    "        keys = sorted(result.keys())\n",
    "        for i in range(1, len(keys)):\n",
    "            x1, x2 = keys[i - 1], keys[i]\n",
    "            y1, y2 = result[x1], result[x2]\n",
    "            # Compute the derivative\n",
    "            deriv = (y2 - y1) / (x2 - x1)\n",
    "            # store derivative for left bound of tupel\n",
    "            derivative[x1] = round(deriv, precision)\n",
    "        derivatives.append(derivative)\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_overlapping_histogram_parts(interval_data, stitching_keys):\n",
    "    for i in range(len(stitching_keys)):\n",
    "        stitching_energy_of_interval_i = stitching_keys[i]\n",
    "\n",
    "        # Modify the i-th interval\n",
    "        current_interval = interval_data[i]\n",
    "        # Keep only keys <= stitching_energy_of_interval_i\n",
    "        current_interval = {k: v for k, v in current_interval.items() if k <= stitching_energy_of_interval_i}\n",
    "\n",
    "        # Modify the (i+1)-th interval if following interval is still in bounds\n",
    "        if i + 1 < len(interval_data):\n",
    "            next_interval = interval_data[i + 1]\n",
    "            # Keep only keys > stitching_energy_of_interval_i\n",
    "            next_interval = {k: v for k, v in next_interval.items() if k > stitching_energy_of_interval_i}\n",
    "\n",
    "        # Update the intervals in the original list\n",
    "        interval_data[i] = current_interval\n",
    "        if i + 1 < len(interval_data):\n",
    "            interval_data[i + 1] = next_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(to_sum):\n",
    "    maxval = max(to_sum)\n",
    "    exp_sum = 0\n",
    "    for value in to_sum:\n",
    "        exp_sum += mp.exp(value-maxval)\n",
    "    res = maxval + mp.log(exp_sum)\n",
    "    return res\n",
    "\n",
    "\n",
    "def free_energy(E_list, log_g_list,  T):\n",
    "    #Need to log sum over g(E)*exp(-E/T) without overflow issues\n",
    "    to_sum = []\n",
    "    for i, log_g in enumerate(log_g_list):\n",
    "        to_sum.append(log_g - E_list[i]/T)\n",
    "    maxval = max(to_sum)\n",
    "    exp_sum = 0\n",
    "    for value in to_sum:\n",
    "        exp_sum += mp.exp(value-maxval)\n",
    "    res = maxval + mp.log(exp_sum)\n",
    "    return -T*res\n",
    "\n",
    "def get_free_energies(rescaled_results,temperatures):\n",
    "    free_energies = []\n",
    "    for seed_results in rescaled_results:\n",
    "        free_energy_classes = []\n",
    "        for error_result in seed_results:\n",
    "            f_values = []\n",
    "            for T in temperatures:\n",
    "                f_values.append(free_energy(error_result[0], error_result[1], T)/(-T))\n",
    "            free_energy_classes.append(f_values)\n",
    "        free_energies.append(free_energy_classes)\n",
    "    return free_energies\n",
    "\n",
    "\n",
    "def process_results(batch_results,X,Y):\n",
    "    rescaled_results = []\n",
    "    for seed_results in batch_results:\n",
    "\n",
    "        rescaled_seed_results = []\n",
    "        for error_result in seed_results:\n",
    "\n",
    "            walker_results = error_result\n",
    "            walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "            walker_results = average_matching_keys(walker_results)\n",
    "            results_x = []\n",
    "            results_y = []\n",
    "            for result in walker_results:\n",
    "                results_y.append(np.array(list(result.values())))\n",
    "                results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "            derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "            minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "            rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "            x_max = -1 -2*X*Y\n",
    "            rescaled_x = []\n",
    "            rescaled_y = []\n",
    "            for i in range(len(results_x)):\n",
    "                for j in range(len(results_x[i])):\n",
    "                    if results_x[i][j] > x_max: #avoid double counting\n",
    "                        x_max = results_x[i][j]\n",
    "                        rescaled_x.append(results_x[i][j])\n",
    "                        rescaled_y.append(results_y[i][j])\n",
    "\n",
    "            offset = log_sum_exp(rescaled_y)\n",
    "            rescaled_y = [res + mp.log(2)*X*Y - offset for res in rescaled_y]\n",
    "            rescaled_seed_results.append([rescaled_x,rescaled_y])\n",
    "        rescaled_results.append(rescaled_seed_results)\n",
    "    return rescaled_results\n",
    "\n",
    "def process_data(data, batch_results, p, size, error):\n",
    "    for entry in data:\n",
    "        histogram_seed = entry[\"histogram_seed\"]\n",
    "        run_seed = entry[\"run_seed\"]\n",
    "        results = entry[\"results\"]\n",
    "\n",
    "        E_list = []\n",
    "        log_g_list = []\n",
    "\n",
    "        # Process the results\n",
    "        for key, value in results.items():\n",
    "            E_list.append(int(key))\n",
    "            log_g_list.append(float(value))\n",
    "\n",
    "        batch_results.append({\n",
    "                'prob': p,\n",
    "                'size': size,\n",
    "                'error': error,\n",
    "                'histogram_seed': histogram_seed,\n",
    "                'run_seed': run_seed,\n",
    "                'E': E_list,\n",
    "                'log_g': log_g_list\n",
    "            })\n",
    "        # offset =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_file(path):\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    content = content.strip().rstrip(',')\n",
    "\n",
    "    corrected_json = f'[{content}]'\n",
    "\n",
    "    try:\n",
    "        data = json.loads(corrected_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_file(filename):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read()\n",
    "            # print(\"file content printout (for debugging):\")\n",
    "            # print(content)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "\n",
    "    # Split content into individual blocks\n",
    "    blocks = content.split('}\\n{')  # Assuming blocks are separated by double newlines\n",
    "\n",
    "    for block in blocks:\n",
    "        # Extract histogram_seed, run_seed, and results\n",
    "        histogram_seed_match = re.search(r'\"histogram_seed\": \"(\\d+)\"', block)\n",
    "        run_seed_match = re.search(r'\"run_seed\": \"(\\d+)\"', block)\n",
    "        results_match = re.search(r'\"results\": \\[([^]]*)\\]', block)\n",
    "\n",
    "        if histogram_seed_match and run_seed_match and results_match:\n",
    "            histogram_seed = histogram_seed_match.group(1)\n",
    "            run_seed = run_seed_match.group(1)\n",
    "            results_str = results_match.group(1)\n",
    "\n",
    "            # Process results\n",
    "            results = {}\n",
    "            results_items = results_str.split(',')\n",
    "            for item in results_items:\n",
    "                key_value = item.split(':')\n",
    "                if len(key_value) == 2:\n",
    "                    key = key_value[0].strip().strip('\"')\n",
    "                    value = float(key_value[1].strip())\n",
    "                    results[key] = value\n",
    "\n",
    "            data.append({\n",
    "                \"histogram_seed\": histogram_seed,\n",
    "                \"run_seed\": run_seed,\n",
    "                \"results\": results\n",
    "            })\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_and_dicts(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"},\\n\")[:-1]\n",
    "\n",
    "    all_results = {}\n",
    "    for block in blocks:\n",
    "        histogram_seed_match = int(re.search(r'\"histogram_seed\": \"(\\d+)\"', block).group(1))\n",
    "        run_seed_match = int(re.search(r'\"run_seed\": \"(\\d+)\"', block).group(1))\n",
    "        results_match = re.search(r'\"results\": \\{([^}]*)\\}', block).group(1)\n",
    "        energy_blocks = results_match.split(',')\n",
    "\n",
    "        energies = []\n",
    "        log_g = []\n",
    "\n",
    "        for e in energy_blocks:\n",
    "            match = re.search(r'\"(-?\\d+)\": (-?\\d+\\.\\d{10})', e)\n",
    "            energies.append(float(match.group(1)))\n",
    "            log_g.append(float(match.group(2)))\n",
    "\n",
    "        last_index = 0\n",
    "\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(1,len(energies)):\n",
    "            if energies[i] < energies[i - 1]:\n",
    "                dict_list.append(dict(zip(energies[last_index:i], log_g[last_index:i])))\n",
    "                last_index = i\n",
    "            if i==(len(energies)-1):\n",
    "                dict_list.append(dict(zip(energies[last_index:], log_g[last_index:])))\n",
    "\n",
    "        all_results[histogram_seed_match] = dict_list\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "boundary_type = \"periodic\"\n",
    "probabilities = [0.1, 0.11, 0.12]\n",
    "size = 4\n",
    "intervals = 10\n",
    "iterations = 1000\n",
    "overlap = 0.25\n",
    "walkers = 8\n",
    "alpha = 0.8\n",
    "beta = 1e-6\n",
    "exchange = 50\n",
    "\n",
    "for p in probabilities:\n",
    "    for error in [\"I\", \"X\", \"Y\", \"Z\"]:\n",
    "\n",
    "        filename = f\"../results/periodic/prob_{p:.6f}/X_{size}_Y_{size}/error_class_{error}/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\"\n",
    "        data = read_results_file(filename)\n",
    "\n",
    "        diffs = []\n",
    "\n",
    "        for d in data:\n",
    "            diff = log_sum_exp(d['results'].values()) - mp.log(2)*4*4\n",
    "            if np.abs(diff) > 1e-10:\n",
    "                diffs.append((p, d['histogram_seed'], diff))\n",
    "\n",
    "        print(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find stitching_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "results_old = read_data_from_file(f\"../results_without_post/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(results_old)\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = get_derivative_wrt_e(walker_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, -16], [2, -12], [5, -8], [5, -8], [6, -4], [8, 0], [8, 4], [9, 4], [9, 8]]\n"
     ]
    }
   ],
   "source": [
    "min_interval_keys = []\n",
    "\n",
    "for i in range(len(derivatives) - 1):\n",
    "    min_diff = np.inf\n",
    "    interval_index = np.inf\n",
    "    min_key = np.inf\n",
    "    for j in range(i+1, len(derivatives)):\n",
    "        overlapping_keys = list(set(derivatives[i].keys()).intersection(set(derivatives[j].keys())))\n",
    "\n",
    "        if len(overlapping_keys) != 0:\n",
    "            for key in overlapping_keys:\n",
    "                diff_deriv = np.abs(derivatives[j][key] - derivatives[i][key])\n",
    "\n",
    "                if diff_deriv < min_diff:\n",
    "                    min_diff = diff_deriv\n",
    "                    interval_index = j\n",
    "                    min_key = key\n",
    "\n",
    "    min_interval_keys.append([interval_index, min_key])\n",
    "\n",
    "print(min_interval_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_stitching_keys = []\n",
    "\n",
    "for i in range(len(min_interval_keys)):\n",
    "    key_interval_i = min_interval_keys[i][1]\n",
    "\n",
    "    check_intersection = True\n",
    "\n",
    "    for j in range(i+1, len(min_interval_keys)):\n",
    "        if min_interval_keys[j][1] <= key_interval_i:\n",
    "            check_intersection = False\n",
    "\n",
    "    if check_intersection:\n",
    "        real_stitching_keys.append(min_interval_keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(real_stitching_keys)):\n",
    "    if (i == 0):\n",
    "        current_interval = walker_results[0]\n",
    "        next_interval = walker_results[real_stitching_keys[0]]\n",
    "\n",
    "        # Concat at\n",
    "        concat = real_stitching_keys[i]\n",
    "    else:\n",
    "        current_interval = real_stitching_keys[i-1]\n",
    "        next_interval = real_stitching_keys[i]\n",
    "\n",
    "        concat = real_stitching_keys[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, -16], [2, -12], [5, -8], [6, -4], [8, 0], [9, 4], [9, 8]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stitching_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1001\n",
    "\n",
    "results_logG = get_seed_and_dicts(f\"../results_get_logG/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rescale_min = get_seed_and_dicts(f\"../results_rescale_minimum/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_intervals = get_seed_and_dicts(f\"../results_rescale_for_concat/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "\n",
    "end_results = get_seed_and_dicts(f\"../results_end/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1001: [{-24.0: 2.0093885885,\n",
       "   -20.0: 4.0973564611,\n",
       "   -16.0: 5.9367450701,\n",
       "   -12.0: 7.4793981539,\n",
       "   -8.0: 8.7596904264,\n",
       "   -4.0: 9.5917784201,\n",
       "   0.0: 9.8525468336,\n",
       "   4.0: 9.5843664633,\n",
       "   8.0: 8.7781111227,\n",
       "   12.0: 7.5048545824,\n",
       "   16.0: 5.9480921255,\n",
       "   20.0: 4.1239861475,\n",
       "   24.0: 2.0454904066}],\n",
       " 1000: [{-24.0: 0.6110980203,\n",
       "   -20.0: 3.3074729135,\n",
       "   -16.0: 5.8848151853,\n",
       "   -12.0: 7.6052999189,\n",
       "   -8.0: 8.7890288522,\n",
       "   -4.0: 9.5545742204,\n",
       "   0.0: 9.8409199407,\n",
       "   4.0: 9.5899986913,\n",
       "   8.0: 8.8021990945,\n",
       "   12.0: 7.6159920385,\n",
       "   16.0: 5.8733665635,\n",
       "   20.0: 3.2664317777,\n",
       "   24.0: 0.5816769292}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000000000015821780041608506556728428327384578650604445483213\n",
      "-0.000000000014667257349131292203367175832101680697259658507381\n"
     ]
    }
   ],
   "source": [
    "for res in end_results:\n",
    "    #print(end_results[res][0].values())\n",
    "    print(np.log(2)*16 - log_sum_exp(end_results[res][0].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After logG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1001\n",
    "results_old = read_data_from_file(f\"../results/periodic_old_result/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "results_logG = get_seed_and_dicts(f\"../results_get_logG/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "for d in range(len(results_old)):\n",
    "    interval_id = int(d/8)\n",
    "\n",
    "    diff = [results_old[d][key] - results_logG[1001][interval_id][key] for key in results_old[d]]\n",
    "\n",
    "    if np.max(np.abs(diff)) != 0:\n",
    "        diffs.append((d,diff))\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rescale_min = get_seed_and_dicts(f\"../results_rescale_minimum/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(results_old)\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "for d in range(len(walker_results)):\n",
    "\n",
    "    diff = [walker_results[d][key] - results_rescale_min[1001][d][key] for key in walker_results[d]]\n",
    "\n",
    "    if np.max(np.abs(diff)) > 1e-9:\n",
    "        diffs.append((d,diff))\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After rescale for concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_type = \"periodic\"\n",
    "probabilities = 0.1\n",
    "size = 4\n",
    "intervals = 10\n",
    "iterations = 1000\n",
    "overlap = 0.25\n",
    "walkers = 8\n",
    "alpha = 0.8\n",
    "beta = 1e-6\n",
    "exchange = 50\n",
    "\n",
    "rescaled_intervals = get_seed_and_dicts(f\"../results_rescale_for_concat/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1001: [{-24.0: 0.0,\n",
       "   -20.0: 2.0879678726,\n",
       "   -16.0: 3.9273564816,\n",
       "   -12.0: 5.4641010761},\n",
       "  {-20.0: 2.1059215069,\n",
       "   -16.0: 3.9273564816,\n",
       "   -12.0: 5.4629769325,\n",
       "   -8.0: 6.7276535034},\n",
       "  {-16.0: 3.9273564816,\n",
       "   -12.0: 5.4700095654,\n",
       "   -8.0: 6.721883297,\n",
       "   -4.0: 7.5087661743},\n",
       "  {-12.0: 0.0, -8.0: 1.2792851925, -4.0: 2.0746188164},\n",
       "  {-12.0: 5.4415910244,\n",
       "   -8.0: 6.721883297,\n",
       "   -4.0: 7.5539712906,\n",
       "   0.0: 7.8297727108},\n",
       "  {-8.0: 1.2640581131,\n",
       "   -4.0: 2.0746188164,\n",
       "   0.0: 2.3433334827,\n",
       "   4.0: 2.1096146107},\n",
       "  {-4.0: 7.5539712906,\n",
       "   0.0: 7.8147397041,\n",
       "   4.0: 7.5465593338,\n",
       "   8.0: 6.7278399467},\n",
       "  {0.0: 1.0414922237, 4.0: 0.7950513363, 8.0: 0.0},\n",
       "  {0.0: 2.3167402744, 4.0: 2.0341844559, 8.0: 1.2480275631, 12.0: 0.0},\n",
       "  {4.0: 2.1096146107,\n",
       "   8.0: 1.3033592701,\n",
       "   12.0: 0.0301027298,\n",
       "   16.0: -1.5266597271,\n",
       "   20.0: -3.3507657051,\n",
       "   24.0: -5.429261446}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1001\n",
    "results_old = read_data_from_file(f\"../results/periodic_old_result/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(results_old)\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "\n",
    "results_x = []\n",
    "results_y = []\n",
    "for result in walker_results:\n",
    "    results_y.append(np.array(list(result.values())))\n",
    "    results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs=[]\n",
    "for i in range(len(results_x)):\n",
    "        diff = [results_y[i][j] - list(rescaled_intervals[seed][i].values())[j] for j in range(len(results_x[i]))]\n",
    "        diffs.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.992007221626409e-14\n",
      "1.0005063444396001e-10\n",
      "1.000231009129493e-10\n",
      "5.442598104500064\n",
      "0.0010070801000665597\n",
      "5.480359554400156\n",
      "0.0010070802000745616\n",
      "6.774254560600127\n",
      "5.535121441000096\n",
      "5.459691286199982\n"
     ]
    }
   ],
   "source": [
    "for d in diffs:\n",
    "    print(np.max(np.abs(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_overlap = get_seed_and_dicts(f\"../results_cut_overlapping/periodic/prob_{probabilities:.6f}/X_{size}_Y_{size}/error_class_X/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1018: [{-24.0: 0.0,\n",
       "   -20.0: 1.0793597698,\n",
       "   -16.0: 3.2492690086,\n",
       "   -12.0: 4.8574972153,\n",
       "   -8.0: 6.2191202641,\n",
       "   -4.0: 6.9108543396,\n",
       "   0.0: 7.3210532665,\n",
       "   4.0: 6.9426600933,\n",
       "   8.0: 6.3018901348,\n",
       "   12.0: 4.91994524,\n",
       "   16.0: 3.3055503368,\n",
       "   20.0: 1.1417651176,\n",
       "   24.0: 0.0349338055}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1018\n",
    "results_old = read_data_from_file(f\"../results/periodic_old_result/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(results_old)\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(walker_results)\n",
    "\n",
    "results_x = []\n",
    "results_y = []\n",
    "for result in walker_results:\n",
    "    results_y.append(np.array(list(result.values())))\n",
    "    results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "\"\"\"Store concatenate interval results\"\"\"\n",
    "concatenated_keys = np.concatenate(results_x)\n",
    "concatenated_values = np.concatenate(results_y)\n",
    "list_of_concat_rescale_dicts = []\n",
    "for keys, values in zip(results_x, results_y):\n",
    "    # Combine keys and values into a dictionary\n",
    "    dict_from_arrays = {k: v for k, v in zip(keys, values)}\n",
    "    list_of_concat_rescale_dicts.append(dict_from_arrays)\n",
    "\n",
    "cut_overlapping_histogram_parts(list_of_concat_rescale_dicts, minimum_deviation_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "og",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
