{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mpmath as mp\n",
    "mp.mp.dps = 50 #higher precision\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.integrate as integrate\n",
    "from plotDensity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(to_sum):\n",
    "    maxval = max(to_sum)\n",
    "    exp_sum = 0\n",
    "    for value in to_sum:\n",
    "        exp_sum += mp.exp(value-maxval)\n",
    "    res = maxval + mp.log(exp_sum)\n",
    "    return res\n",
    "\n",
    "\n",
    "def free_energy(E_list, log_g_list,  T):\n",
    "    #Need to log sum over g(E)*exp(-E/T) without overflow issues\n",
    "    to_sum = []\n",
    "    for i, log_g in enumerate(log_g_list):\n",
    "        to_sum.append(log_g - E_list[i]/T)\n",
    "    maxval = max(to_sum)\n",
    "    exp_sum = 0\n",
    "    for value in to_sum:\n",
    "        exp_sum += mp.exp(value-maxval)\n",
    "    res = maxval + mp.log(exp_sum)\n",
    "    return -T*res\n",
    "\n",
    "def get_free_energies(rescaled_results,temperatures):\n",
    "    free_energies = []\n",
    "    for seed_results in rescaled_results:\n",
    "        free_energy_classes = []\n",
    "        for error_result in seed_results:\n",
    "            f_values = []\n",
    "            for T in temperatures:\n",
    "                f_values.append(free_energy(error_result[0], error_result[1], T)/(-T))\n",
    "            free_energy_classes.append(f_values)\n",
    "        free_energies.append(free_energy_classes)\n",
    "    return free_energies\n",
    "\n",
    "\n",
    "def process_results(batch_results,X,Y):\n",
    "    rescaled_results = []\n",
    "    for seed_results in batch_results:\n",
    "\n",
    "        rescaled_seed_results = []\n",
    "        for error_result in seed_results:\n",
    "\n",
    "            walker_results = error_result\n",
    "            walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "            walker_results = average_matching_keys(walker_results)\n",
    "            results_x = []\n",
    "            results_y = []\n",
    "            for result in walker_results:\n",
    "                results_y.append(np.array(list(result.values())))\n",
    "                results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "            derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "            minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "            rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "            x_max = -1 -2*X*Y\n",
    "            rescaled_x = []\n",
    "            rescaled_y = []\n",
    "            for i in range(len(results_x)):\n",
    "                for j in range(len(results_x[i])):\n",
    "                    if results_x[i][j] > x_max: #avoid double counting\n",
    "                        x_max = results_x[i][j]\n",
    "                        rescaled_x.append(results_x[i][j])\n",
    "                        rescaled_y.append(results_y[i][j])\n",
    "\n",
    "            offset = log_sum_exp(rescaled_y)\n",
    "            rescaled_y = [res + mp.log(2)*X*Y - offset for res in rescaled_y]\n",
    "            rescaled_seed_results.append([rescaled_x,rescaled_y])\n",
    "        rescaled_results.append(rescaled_seed_results)\n",
    "    return rescaled_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read()\n",
    "            # print(\"file content printout (for debugging):\")\n",
    "            # print(content)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "\n",
    "    # Split content into individual blocks\n",
    "    blocks = content.split('}\\n{')  # Assuming blocks are separated by double newlines\n",
    "\n",
    "    for block in blocks:\n",
    "        # Extract histogram_seed, run_seed, and results\n",
    "        histogram_seed_match = re.search(r'\"histogram_seed\": \"(\\d+)\"', block)\n",
    "        run_seed_match = re.search(r'\"run_seed\": \"(\\d+)\"', block)\n",
    "        results_match = re.search(r'\"results\": \\[([^]]*)\\]', block)\n",
    "\n",
    "        if histogram_seed_match and run_seed_match and results_match:\n",
    "            histogram_seed = histogram_seed_match.group(1)\n",
    "            run_seed = run_seed_match.group(1)\n",
    "            results_str = results_match.group(1)\n",
    "\n",
    "            # Process results\n",
    "            results = {}\n",
    "            results_items = results_str.split(',')\n",
    "            for item in results_items:\n",
    "                key_value = item.split(':')\n",
    "                if len(key_value) == 2:\n",
    "                    key = key_value[0].strip().strip('\"')\n",
    "                    value = float(key_value[1].strip())\n",
    "                    results[key] = value\n",
    "\n",
    "            data.append({\n",
    "                \"histogram_seed\": histogram_seed,\n",
    "                \"run_seed\": run_seed,\n",
    "                \"results\": results\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_data(data, batch_results, p, size, error):\n",
    "    for entry in data:\n",
    "        histogram_seed = entry[\"histogram_seed\"]\n",
    "        run_seed = entry[\"run_seed\"]\n",
    "        results = entry[\"results\"]\n",
    "\n",
    "        E_list = []\n",
    "        log_g_list = []\n",
    "\n",
    "        # Process the results\n",
    "        for key, value in results.items():\n",
    "            E_list.append(int(key))\n",
    "            log_g_list.append(float(value))\n",
    "\n",
    "        batch_results.append({\n",
    "                'prob': p,\n",
    "                'size': size,\n",
    "                'error': error,\n",
    "                'histogram_seed': histogram_seed,\n",
    "                'run_seed': run_seed,\n",
    "                'E': E_list,\n",
    "                'log_g': log_g_list\n",
    "            })\n",
    "        # offset = log_sum_exp(log_g_list)\n",
    "        # print('deviation from sqrt(2)xy: ', mp.log(2)*size*size - offset, ' log sum exp:', offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\n"
     ]
    }
   ],
   "source": [
    "boundary_type = \"periodic\"\n",
    "batch_results = []\n",
    "probabilities = [0.1]\n",
    "sizes = [4]\n",
    "intervals = 10\n",
    "iterations = 1000\n",
    "overlap = 0.25\n",
    "walkers = 8\n",
    "alpha = 0.8\n",
    "beta = 1e-6\n",
    "exchange = 50\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "for p in probabilities:\n",
    "    for size in sizes:\n",
    "        for error in [\"I\"]:\n",
    "            filename = f\"../results/periodic_head_old_stitched_handling/prob_{p:.6f}/X_{size}_Y_{size}/error_class_{error}/StitchedHistogram__intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}exchange_offset{exchange}.txt\"\n",
    "            data = parse_file(filename)\n",
    "            if data:\n",
    "                process_data(data, batch_results, p, size, error)\n",
    "print(len(batch_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/periodic_head_old_stitched_handling/prob_0.100000/X_4_Y_4/error_class_I/StitchedHistogram__intervals_10_iterations_1000_overlap_0.250000_walkers_8_alpha_0.800000_beta_0.0000010000exchange_offset50.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../results/periodic_head_old_stitched_handling/prob_0.100000/X_4_Y_4/error_class_I/StitchedHistogram__intervals_10_iterations_1000_overlap_0.250000_walkers_8_alpha_0.800000_beta_0.0000010000exchange_offset50.txt'\n",
    "data = []\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    content = file.read()\n",
    "    # print(\"file content printout (for debugging):\")\n",
    "    # print(content)\n",
    "\n",
    "# Split content into individual blocks\n",
    "blocks = content.split('}\\n{')  # Assuming blocks are separated by double newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.zeros(2000)\n",
    "\n",
    "without_duplicates = []\n",
    "\n",
    "for i in range(len(blocks)):\n",
    "    histogram_seed_match = re.search(r'\"histogram_seed\": \"(\\d+)\"', blocks[i])\n",
    "    hist_seed = int(histogram_seed_match.group(1))\n",
    "    if check[hist_seed-1] == 0:\n",
    "        without_duplicates.append(blocks[i])\n",
    "        check[hist_seed-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = \"output.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write each string in the list to the file\n",
    "    for entry in without_duplicates:\n",
    "        file.write(entry)\n",
    "        file.write(\"}\\n\")\n",
    "        file.write(\"{\")\n",
    "\n",
    "print(f\"Data written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# only keep results which come with all different error types:\n",
    "#Changing structure to match postprocessing and plotting\n",
    "\n",
    "grouped_results = defaultdict(list)\n",
    "for result in batch_results:\n",
    "    key = (result['prob'], result['size'], result['histogram_seed'])\n",
    "    grouped_results[key].append(result)\n",
    "\n",
    "filtered_results = defaultdict(list)\n",
    "for key, results in grouped_results.items():\n",
    "    newkey = (key[0],key[1])\n",
    "    errors = set(result['error'] for result in results)\n",
    "    if errors == {'I', 'X', 'Y', 'Z'}:\n",
    "        #To be removed once normalization is properly handled in c\n",
    "        for result in results:\n",
    "            log_g_list = result['log_g']\n",
    "            offset = log_sum_exp(log_g_list)\n",
    "            rescaled_log_g_list = [res + mp.log(2)*size*size - offset for res in log_g_list]\n",
    "            #print(mp.log(2)*size*size - offset)\n",
    "            result['log_g'] = rescaled_log_g_list\n",
    "        filtered_results[newkey].append([[result['E'], result['log_g']] for result in results])\n",
    "    else:\n",
    "        print(f\"has issue with an error class prob: {key[0]} size: {key[1]} interaction seed: {key[2]} available errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "og",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
