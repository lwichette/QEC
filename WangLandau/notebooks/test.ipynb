{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotDensity import *\n",
    "import mpmath as mp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(to_sum):\n",
    "    maxval = max(to_sum)\n",
    "    exp_sum = 0\n",
    "    for value in to_sum:\n",
    "        exp_sum += mp.exp(value-maxval)\n",
    "    res = maxval + mp.log(exp_sum)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_file(path):\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    content = content.strip().rstrip(',')\n",
    "\n",
    "    corrected_json = f'[{content}]'\n",
    "\n",
    "    try:\n",
    "        data = json.loads(corrected_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_overlapping_histogram_parts(interval_data, stitching_keys):\n",
    "    for i in range(len(stitching_keys)):\n",
    "        stitching_energy_of_interval_i = stitching_keys[i]\n",
    "\n",
    "        # Modify the i-th interval\n",
    "        current_interval = interval_data[i]\n",
    "        # Keep only keys <= stitching_energy_of_interval_i\n",
    "        current_interval = {k: v for k, v in current_interval.items() if k <= stitching_energy_of_interval_i}\n",
    "\n",
    "        # Modify the (i+1)-th interval if following interval is still in bounds\n",
    "        if i + 1 < len(interval_data):\n",
    "            next_interval = interval_data[i + 1]\n",
    "            # Keep only keys > stitching_energy_of_interval_i\n",
    "            next_interval = {k: v for k, v in next_interval.items() if k > stitching_energy_of_interval_i}\n",
    "\n",
    "        # Update the intervals in the original list\n",
    "        interval_data[i] = current_interval\n",
    "        if i + 1 < len(interval_data):\n",
    "            interval_data[i + 1] = next_interval\n",
    "\n",
    "def process_data(data, batch_results, p, size, error):\n",
    "    for entry in data:\n",
    "        histogram_seed = entry[\"histogram_seed\"]\n",
    "        run_seed = entry[\"run_seed\"]\n",
    "        results = entry[\"results\"]\n",
    "\n",
    "        E_list = []\n",
    "        log_g_list = []\n",
    "\n",
    "        # Process the results\n",
    "        for key, value in results.items():\n",
    "            E_list.append(int(key))\n",
    "            log_g_list.append(float(value))\n",
    "\n",
    "        batch_results.append({\n",
    "                'prob': p,\n",
    "                'size': size,\n",
    "                'error': error,\n",
    "                'histogram_seed': histogram_seed,\n",
    "                'run_seed': run_seed,\n",
    "                'E': E_list,\n",
    "                'log_g': log_g_list\n",
    "            })\n",
    "        # offset ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 4\n",
    "Y = 4\n",
    "\n",
    "num_walker = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_type = \"periodic\"\n",
    "batch_results = []\n",
    "probabilities = [0.1]\n",
    "sizes = [4]\n",
    "intervals = 5\n",
    "iterations = 1000\n",
    "overlap = 0.25\n",
    "walkers = 8\n",
    "alpha = 0.8\n",
    "beta = 1e-6\n",
    "exchange = 50\n",
    "error = \"X\"\n",
    "p = 0.1\n",
    "batch_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "diffs = {}\n",
    "\n",
    "for seed in range(1000, 2000):\n",
    "    results_new = read_data_from_file(f\"../results/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "    results_old = read_data_from_file(f\"../results/periodic_1/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    results_new = results_new[0::num_walker]\n",
    "    results_old = results_old[0::num_walker]\n",
    "\n",
    "    for i in range(len(results_new)):\n",
    "        diff = [results_new[i][key] - results_old[i][key] for key in results_new[i]]\n",
    "        if np.max(np.abs(diff)) > 1e-05:\n",
    "            diffs[seed] = (i, diff)\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "max_all = []\n",
    "\n",
    "for seed in range(1000, 1150):\n",
    "    before_run_1 = read_data_from_file(f\"../results_before_run_13/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "    before_run_2 = read_data_from_file(f\"../results_before_run_33/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    diffs = []\n",
    "    for i in range(len(before_run_1)):\n",
    "        diff = [before_run_1[i][key] - before_run_2[i][key] for key in before_run_1[i]]\n",
    "        diffs.append(diff)\n",
    "\n",
    "    max = []\n",
    "\n",
    "    for d in diffs:\n",
    "        max.append(np.max(np.abs(d)))\n",
    "\n",
    "    max_all.append(np.max(max))\n",
    "\n",
    "print(np.max(max_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n"
     ]
    }
   ],
   "source": [
    "max_all = []\n",
    "\n",
    "for seed in range(1000, 1150):\n",
    "    after_run_1 = read_data_from_file(f\"../results_after_run_13/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "    after_run_2 = read_data_from_file(f\"../results_after_run_33/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    diffs = []\n",
    "    for i in range(len(after_run_1)):\n",
    "        diff = [after_run_1[i][key] - after_run_2[i][key] for key in after_run_1[i]]\n",
    "        diffs.append(diff)\n",
    "\n",
    "    max = []\n",
    "\n",
    "    for d in diffs:\n",
    "        max.append(np.max(np.abs(d)))\n",
    "\n",
    "    max_all.append((seed, np.max(max)))\n",
    "\n",
    "for m in max_all:\n",
    "    if m[1] != 0:\n",
    "        print(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dicts(dicts):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    sums = defaultdict(float)\n",
    "\n",
    "    # Dictionary to count occurrences of each key\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    # Loop through each dictionary\n",
    "    for d in dicts:\n",
    "        for key, value in d.items():\n",
    "            sums[key] += value\n",
    "            counts[key] += 1\n",
    "\n",
    "    # Calculate the averages\n",
    "    averages = {key: sums[key] / counts[key] for key in sums}\n",
    "\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1140\n",
    "\n",
    "before_seed = read_data_from_file(f\"../results_before_run_33/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "for i in range(8, len(before_seed) + 8, 8):\n",
    "    eight_walker = before_seed[(i-8):i]\n",
    "    averages.append(average_dicts(eight_walker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1140\n",
    "\n",
    "patient_zero = read_data_from_file(f\"../results_after_run_13/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, [0.0, 0.0, -1.0625, -2.59375]), (6, [-6.1875, 3.09375, 1.1875, 1.90625]), (7, [8.3125, -1.40625, -2.5625, -4.34375])]\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "for walker in range(len(patient_zero)):\n",
    "    interval_id = int(walker/8)\n",
    "    diff = [patient_zero[walker][key] - averages[interval_id][key] for key in patient_zero[walker]]\n",
    "\n",
    "    if np.max(np.abs(diff)!=0):\n",
    "        diffs.append((walker, diff))\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-22: 489.0625, -18: 500.03125, -14: 504.0625, -10: 506.84375},\n",
       " {-22: 489.0625, -18: 500.03125, -14: 503.0, -10: 504.25},\n",
       " {-22: 482.875, -18: 503.125, -14: 505.25, -10: 508.75},\n",
       " {-22: 497.375, -18: 498.625, -14: 501.5, -10: 502.5},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-18: 622.75, -14: 625.078125, -10: 627.171875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-14: 623.0, -10: 625.125, -6: 626.875},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 624.453125, -6: 625.15625, -2: 625.390625},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-10: 467.390625, -6: 468.65625, -2: 469.28125, 2: 469.671875},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-6: 624.75, -2: 625.140625, 2: 625.109375},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {-2: 625.03125, 2: 625.34375, 6: 624.625},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 626.15625, 6: 625.015625, 10: 623.828125},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {2: 470.484375, 6: 469.984375, 10: 468.4375, 14: 466.09375},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875},\n",
       " {6: 556.65625, 10: 553.59375, 14: 550.65625, 18: 548.125, 22: 540.96875}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4]), array([20222, 20226, 20230, 20222, 20226, 20230, 20222, 20226, 20230,\n",
      "       20087, 20091, 20095, 20099, 20222, 20226, 20230]))\n"
     ]
    }
   ],
   "source": [
    "first_run = []\n",
    "\n",
    "with open('../output.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        averages = line.strip().split(' ')[1:]\n",
    "        first_run.append(list(map(float, averages)))\n",
    "\n",
    "second_run = []\n",
    "\n",
    "with open('../output_1.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        averages = line.strip().split(' ')[1:]\n",
    "        second_run.append(list(map(float, averages)))\n",
    "\n",
    "diff_averages = np.abs(np.array(first_run) - np.array(second_run))\n",
    "\n",
    "non_zero_indices = np.nonzero(diff_averages)\n",
    "\n",
    "print(non_zero_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 1, 1, 1, 1, 1, 1]), array([20087, 20091, 20095, 20099, 20222, 20226, 20230]))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.625 364.219\n",
      "374.375 373.75\n",
      "379.25 377.484\n",
      "385.75 385.844\n",
      "488.125 483.0\n",
      "501.75 503.594\n",
      "510.125 511.906\n"
     ]
    }
   ],
   "source": [
    "first_run_array = np.array(first_run)\n",
    "second_run_array = np.array(second_run)\n",
    "\n",
    "for i in range(len(non_zero_indices[0])):\n",
    "    print(first_run_array[non_zero_indices[0][i], non_zero_indices[1][i]], second_run_array[non_zero_indices[0][i], non_zero_indices[1][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525.609"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_run_array[4, 20087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msecond_run\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20222\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "print(np.arraysecond_run[1,20222])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../script.log', 'r') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = content.split('\\n')\n",
    "\n",
    "end_runs = []\n",
    "\n",
    "for l in range(len(lines)):\n",
    "    if \"Total\" in lines[l]:\n",
    "        end_runs.append(l)\n",
    "\n",
    "first_run = lines[:end_runs[0]-3]\n",
    "second_run = lines[end_runs[0]+1:end_runs[1]-3]\n",
    "\n",
    "for l in range(len(first_run)):\n",
    "    # Use regular expressions to extract the energy and the interval\n",
    "    first_match = re.search(r\"energy: (-?\\d+) for interval \\[(-?\\d+), (-?\\d+)\\]\", first_run[l])\n",
    "    first_energy = int(first_match.group(1))\n",
    "    first_interval = (int(first_match.group(2)), int(first_match.group(3)))\n",
    "\n",
    "    second_match = re.search(r\"energy: (-?\\d+) for interval \\[(-?\\d+), (-?\\d+)\\]\", second_run[l])\n",
    "    second_energy = int(second_match.group(1))\n",
    "    second_interval = (int(second_match.group(2)), int(second_match.group(3)))\n",
    "\n",
    "    if (second_interval == first_interval):\n",
    "        diff = second_energy - first_energy\n",
    "        if diff!=0:\n",
    "            print(\"HELLO\")\n",
    "    else:\n",
    "        print(\"ALARM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Consistency old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = {}\n",
    "\n",
    "diff_between_walker = []\n",
    "for seed in range(1000, 1150):\n",
    "    results_new = read_data_from_file(f\"../results/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    for i in range(10):\n",
    "        first_walker = results_new[offset]\n",
    "        for j in range(1, 8):\n",
    "            second_walker = results_new[offset +1]\n",
    "\n",
    "            diff = [first_walker[key] - second_walker[key] for key in first_walker]\n",
    "\n",
    "            if np.max(np.abs(diff) != 0):\n",
    "                diff_between_walker.append((seed, i, j, diff))\n",
    "        offset += 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_between_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = {}\n",
    "\n",
    "for seed in range(1000, 1150):\n",
    "    results_new = read_data_from_file(f\"../results/periodic_1/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "    results_old = read_data_from_file(f\"../results/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    results_new = results_new[0::num_walker]\n",
    "    results_old = results_old[0::num_walker]\n",
    "\n",
    "    for i in range(len(results_new)):\n",
    "        diff = [results_new[i][key] - results_old[i][key] for key in results_new[i]]\n",
    "        if np.max(np.abs(diff)) > 1e-05:\n",
    "            diffs[seed] = (i, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1115: (0, [10.875, 4.6875, -8.75, -8.71875])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Consistency new Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_and_dicts(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"},\\n\")[:-1]\n",
    "\n",
    "    all_results = {}\n",
    "    for block in blocks:\n",
    "        histogram_seed_match = int(re.search(r'\"histogram_seed\": \"(\\d+)\"', block).group(1))\n",
    "        run_seed_match = int(re.search(r'\"run_seed\": \"(\\d+)\"', block).group(1))\n",
    "        results_match = re.search(r'\"results\": \\{([^}]*)\\}', block).group(1)\n",
    "        energy_blocks = results_match.split(',')\n",
    "\n",
    "        energies = []\n",
    "        log_g = []\n",
    "\n",
    "        for e in energy_blocks:\n",
    "            match = re.search(r'\"(-?\\d+)\": (\\d+\\.\\d{10})', e)\n",
    "            energies.append(float(match.group(1)))\n",
    "            log_g.append(float(match.group(2)))\n",
    "\n",
    "        last_index = 0\n",
    "\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(1,len(energies)):\n",
    "            if energies[i] < energies[i - 1]:\n",
    "                dict_list.append(dict(zip(energies[last_index:i], log_g[last_index:i])))\n",
    "                last_index = i\n",
    "            if i==(len(energies)-1):\n",
    "                dict_list.append(dict(zip(energies[last_index:], log_g[last_index:])))\n",
    "\n",
    "        all_results[histogram_seed_match] = dict_list\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_new = f\"../results/periodic_just_now/prob_{p:.6f}/X_{sizes[0]}_Y_{sizes[0]}/error_class_{error}/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\"\n",
    "file_old = f\"../results/periodic/prob_{p:.6f}/X_{sizes[0]}_Y_{sizes[0]}/error_class_{error}/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\"\n",
    "\n",
    "results_new = get_seed_and_dicts(file_new)\n",
    "results_old = get_seed_and_dicts(file_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "\n",
    "for i in range(1000, 1200):\n",
    "    entry_new = results_new[i]\n",
    "    entry_old = results_old[i]\n",
    "\n",
    "    for j in range(len(entry_new)):\n",
    "        result = [entry_new[j][key] - entry_old[j][key] for key in entry_new[j]]\n",
    "        diffs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_final = []\n",
    "for d in diffs:\n",
    "    if np.max(np.abs(d)) > 0:\n",
    "        diffs_final.append(np.max(np.abs(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08608913420000075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(diffs_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m wrong_seeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocks:\n\u001b[0;32m---> 13\u001b[0m   histogram_seed_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistogram_seed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m   run_seed_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, block)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m   results_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m([^}]*)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, block)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "filename = f\"../results/periodic_error/prob_{p:.6f}/X_{sizes[0]}_Y_{sizes[0]}/error_class_{error}/StitchedHistogram_intervals_{intervals}_iterations_{iterations}_overlap_{overlap:.6f}_walkers_{walkers}_alpha_{alpha:.6f}_beta_{beta:.10f}_exchange_offset{exchange}.txt\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "\n",
    "blocks = content.split(\"},\\n\")\n",
    "\n",
    "## Loop over new code and get values\n",
    "wrong_seeds = []\n",
    "\n",
    "for block in blocks:\n",
    "  histogram_seed_match = int(re.search(r'\"histogram_seed\": \"(\\d+)\"', block).group(1))\n",
    "  run_seed_match = int(re.search(r'\"run_seed\": \"(\\d+)\"', block).group(1))\n",
    "  results_match = re.search(r'\"results\": \\{([^}]*)\\}', block).group(1)\n",
    "  energy_blocks = results_match.split(',')\n",
    "\n",
    "  energies = []\n",
    "  log_g = []\n",
    "\n",
    "  for e in energy_blocks:\n",
    "      match = re.search(r'\"(-?\\d+)\": (\\d+\\.\\d{10})', e)\n",
    "      energies.append(float(match.group(1)))\n",
    "      log_g.append(float(match.group(2)))\n",
    "\n",
    "  last_index = 0\n",
    "\n",
    "  dict_list = []\n",
    "\n",
    "  for i in range(1,len(energies)):\n",
    "    if energies[i] < energies[i - 1]:\n",
    "      dict_list.append(dict(zip(energies[last_index:i], log_g[last_index:i])))\n",
    "      last_index = i\n",
    "    if i==(len(energies)-1):\n",
    "      dict_list.append(dict(zip(energies[last_index:], log_g[last_index:])))\n",
    "\n",
    "  ## Read in old values\n",
    "  old_result_handling_out = read_data_from_file(f\"../results/periodic/prob_0.100000/X_4_Y_4/seed_{histogram_seed_match}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "  \"\"\"averages over walker results per intervals\"\"\"\n",
    "  walker_results = average_matching_keys(old_result_handling_out)\n",
    "\n",
    "  \"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "  walker_results = get_renormalized_log_g_values_as_dict_list(old_result_handling_out)\n",
    "\n",
    "  walker_results = walker_results[0::num_walker]\n",
    "\n",
    "  for i in range(len(walker_results)):\n",
    "      if walker_results[i].keys() == dict_list[i].keys():\n",
    "          diff = np.array([walker_results[i][key] for key in sorted(walker_results[i].keys())]) - np.array([dict_list[i][key] for key in sorted(dict_list[i].keys())])\n",
    "          if np.max(diff) > 1e-5:\n",
    "            wrong_seeds.append((histogram_seed_match, i, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 0, array([ 0.        ,  0.000175  , -0.00144148]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_seeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{-18: 0.0, -14: 2.3060755728999993, -10: 3.7558109760000207}, {-14: 0.0, -10: 1.4316029548000415, -6: 2.420887946999983}, {-14: 0.0, -10: 1.4884452820000433, -6: 2.547107696600051}, {-10: 0.0, -6: 0.9729471207000415, -2: 1.4316787720000548}, {-10: 0.0, -6: 1.0570590495999568, -2: 1.530307769799947}, {-6: 0.0, -2: 0.47184920309996414, 2: 0.49166107179996743}, {-6: 0.0, -2: 0.4978125095000223, 2: 0.5081222057000332}, {-2: 0.5148189067999738, 2: 0.5105583667999554, 6: 0.0}, {-2: 0.46153950689995327, 2: 0.4701387881999608, 6: 0.0}, {2: 5.26059389119996, 6: 4.79037141799995, 10: 3.776184320499965, 14: 2.30865025519995, 18: 0.0}]\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "seed = 1101\n",
    "\n",
    "old_result_handling_out = read_data_from_file(f\"../results/periodic/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(old_result_handling_out)\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(old_result_handling_out)\n",
    "\n",
    "walker_results = walker_results[0::num_walker]\n",
    "\n",
    "print(walker_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(walker_results)):\n",
    "    if walker_results[i].keys() == dict_list[i].keys():\n",
    "        diff = np.array([walker_results[i][key] for key in sorted(walker_results[i].keys())]) - np.array([dict_list[i][key] for key in sorted(dict_list[i].keys())])\n",
    "        if np.max(diff) > 1e-9:\n",
    "            print(np.max(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 4.853178024300007, 12: 3.6080551147999813, 16: 1.8163652420000176, 20: 0.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walker_results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.802430000669602e-05"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list[i][8] - walker_results[i][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(dict_values([0.0, 1.8498768807000943, 3.64570975310005]),\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(walker_results[i].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"normalize the walker results by min value for log results\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m walker_results \u001b[38;5;241m=\u001b[39m get_renormalized_log_g_values_as_dict_list(old_result_handling_out)\n\u001b[0;32m---> 12\u001b[0m walker_results \u001b[38;5;241m=\u001b[39m walker_results[\u001b[38;5;241m0\u001b[39m::\u001b[43mnum\u001b[49m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(walker_results)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num' is not defined"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "\n",
    "for seed in range(1000, 1200):\n",
    "    old_result_handling_out = read_data_from_file(f\"../results/old_result/prob_0.100000/X_4_Y_4/seed_{seed}/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "    \"\"\"averages over walker results per intervals\"\"\"\n",
    "    walker_results = average_matching_keys(old_result_handling_out)\n",
    "\n",
    "    \"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "    walker_results = get_renormalized_log_g_values_as_dict_list(old_result_handling_out)\n",
    "\n",
    "    walker_results = walker_results[0::num_walker]\n",
    "    print(walker_results)\n",
    "    break\n",
    "    # results_x = []\n",
    "    # results_y = []\n",
    "    # for result in walker_results:\n",
    "    #     results_y.append(np.array(list(result.values())))\n",
    "    #     results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "    # derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "    # minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "    # rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "    # \"\"\"Store concatenate interval results\"\"\"\n",
    "    # concatenated_keys = np.concatenate(results_x)\n",
    "    # concatenated_values = np.concatenate(results_y)\n",
    "    # list_of_concat_rescale_dicts = []\n",
    "    # for keys, values in zip(results_x, results_y):\n",
    "    #     # Combine keys and values into a dictionary\n",
    "    #     dict_from_arrays = {k: v for k, v in zip(keys, values)}\n",
    "    #     list_of_concat_rescale_dicts.append(dict_from_arrays)\n",
    "\n",
    "    # cut_overlapping_histogram_parts(list_of_concat_rescale_dicts, minimum_deviation_energies)\n",
    "\n",
    "    # # Flatten the list of dictionaries into a single list of values\n",
    "    # cut_values = [value for d in list_of_concat_rescale_dicts for value in d.values()]\n",
    "\n",
    "    # # Convert the list of values into a NumPy array\n",
    "    # cut_values_array = np.array(cut_values)\n",
    "\n",
    "    # offset = log_sum_exp(cut_values_array)\n",
    "    # rescaled_y = [res + mp.log(2)*16 - offset for res in cut_values_array]\n",
    "\n",
    "    # diffs.append(log_sum_exp(rescaled_y) - np.log(2)*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpf('1.7763568394002505e-15')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(walker_results)\n",
    "\n",
    "results_x = []\n",
    "results_y = []\n",
    "for result in walker_results:\n",
    "    results_y.append(np.array(list(result.values())))\n",
    "    results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "\"\"\"Store concatenate interval results\"\"\"\n",
    "concatenated_keys = np.concatenate(results_x)\n",
    "concatenated_values = np.concatenate(results_y)\n",
    "list_of_concat_rescale_dicts = []\n",
    "for keys, values in zip(results_x, results_y):\n",
    "    # Combine keys and values into a dictionary\n",
    "    dict_from_arrays = {k: v for k, v in zip(keys, values)}\n",
    "    list_of_concat_rescale_dicts.append(dict_from_arrays)\n",
    "\n",
    "cut_overlapping_histogram_parts(list_of_concat_rescale_dicts, minimum_deviation_energies)\n",
    "\n",
    "# # Flatten the list of dictionaries into a single list of values\n",
    "# cut_values = [value for d in list_of_concat_rescale_dicts for value in d.values()]\n",
    "\n",
    "# # Convert the list of values into a NumPy array\n",
    "# cut_values_array = np.array(cut_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is set to e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = (4,4)\n",
    "\n",
    "\"\"\"Incoming data to get handled by new result handling function\"\"\"\n",
    "interval_data = [\n",
    "  627.1095947266,\n",
    "  628.8754638672,\n",
    "  630.9088623047,\n",
    "  632.5491699219,\n",
    "  633.5110107422,\n",
    "  404.7702880859,\n",
    "  406.3506591797,\n",
    "  407.3104492188,\n",
    "  407.8216796875,\n",
    "  407.8045410156,\n",
    "  404.0828857422,\n",
    "  405.1276611328,\n",
    "  405.6312011719,\n",
    "  405.6431152344,\n",
    "  405.1499023438,\n",
    "  401.4141113281,\n",
    "  401.9133789062,\n",
    "  401.9149902344,\n",
    "  401.4306152344,\n",
    "  400.5290527344,\n",
    "  402.1429199219,\n",
    "  402.1175537109,\n",
    "  401.6156250000,\n",
    "  400.6624267578,\n",
    "  399.0767089844,\n",
    "  613.2126708984,\n",
    "  612.2446777344,\n",
    "  610.6416259766,\n",
    "  608.6352050781,\n",
    "  606.9015625000\n",
    "]\n",
    "\n",
    "\"\"\"data after applying min rescaling in new result handling function\"\"\"\n",
    "min_rescaled_data = [\n",
    "  0.0000000000,\n",
    "  1.7658691406,\n",
    "  3.7992675781,\n",
    "  5.4395751953,\n",
    "  6.4014160156,\n",
    "  0.0000000000,\n",
    "  1.5803710938,\n",
    "  2.5401611328,\n",
    "  3.0513916016,\n",
    "  3.0342529297,\n",
    "  0.0000000000,\n",
    "  1.0447753906,\n",
    "  1.5483154297,\n",
    "  1.5602294922,\n",
    "  1.0670166016,\n",
    "  0.8850585938,\n",
    "  1.3843261719,\n",
    "  1.3859375000,\n",
    "  0.9015625000,\n",
    "  0.0000000000,\n",
    "  3.0662109375,\n",
    "  3.0408447266,\n",
    "  2.5389160156,\n",
    "  1.5857177734,\n",
    "  0.0000000000,\n",
    "  6.3111083984,\n",
    "  5.3431152344,\n",
    "  3.7400634766,\n",
    "  1.7336425781,\n",
    "  0.0000000000\n",
    "]\n",
    "\n",
    "\"\"\"data after rescaling for stitching intervals\"\"\"\n",
    "stitched_data = [\n",
    "  0.0000000000,\n",
    "  1.7658691406,\n",
    "  3.7992675781,\n",
    "  5.4395751953,\n",
    "  6.4014160156,\n",
    "  3.8592041016,\n",
    "  5.4395751953,\n",
    "  6.3993652344,\n",
    "  6.9105957031,\n",
    "  6.8934570313,\n",
    "  5.3545898438,\n",
    "  6.3993652344,\n",
    "  6.9029052734,\n",
    "  6.9148193359,\n",
    "  6.4216064453,\n",
    "  6.3993652344,\n",
    "  6.8986328125,\n",
    "  6.9002441406,\n",
    "  6.4158691406,\n",
    "  5.5143066406,\n",
    "  6.9256103516,\n",
    "  6.9002441406,\n",
    "  6.3983154297,\n",
    "  5.4451171875,\n",
    "  3.8593994141,\n",
    "  6.3983154297,\n",
    "  5.4303222656,\n",
    "  3.8272705078,\n",
    "  1.8208496094,\n",
    "  0.0872070313\n",
    "]\n",
    "\n",
    "cut_data = {\n",
    "  \"-22\": 0.0000000000,\n",
    "  \"-18\": 1.7658691406,\n",
    "  \"-14\": 3.7992675781,\n",
    "  \"-10\": 5.4395751953,\n",
    "  \"-6\": 6.3993652344,\n",
    "  \"-2\": 6.8986328125,\n",
    "  \"2\": 6.9002441406,\n",
    "  \"6\": 6.3983154297,\n",
    "  \"10\": 5.4303222656,\n",
    "  \"14\": 3.8272705078,\n",
    "  \"18\": 1.8208496094,\n",
    "  \"22\": 0.0872070312\n",
    "}\n",
    "\n",
    "normalized_data = {\n",
    "  \"-22\": 2.8611055117,\n",
    "  \"-18\": 4.6269746524,\n",
    "  \"-14\": 6.6603730899,\n",
    "  \"-10\": 8.3006807071,\n",
    "  \"-6\": 9.2604707461,\n",
    "  \"-2\": 9.7597383242,\n",
    "  \"2\": 9.7613496524,\n",
    "  \"6\": 9.2594209414,\n",
    "  \"10\": 8.2914277774,\n",
    "  \"14\": 6.6883760196,\n",
    "  \"18\": 4.6819551211,\n",
    "  \"22\": 2.9483125430\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_result_handling_out = read_data_from_file(\"../results/periodic/prob_0.100000/X_4_Y_4/seed_1001/error_class_X/intervals_10_iterations_1000_overlap_0.250000_walkers_8_seed_run_42_alpha_0.800000_beta_0.0000010000exchange_offset50.txt\")\n",
    "\n",
    "\"\"\"Reducing to single walker resutl per interval\"\"\"\n",
    "walker_results = average_matching_keys(old_result_handling_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_result_handling_out_concatenated = [list(d.values()) for d in walker_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION INPUT RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(abs((np.concatenate(old_result_handling_out_concatenated)) - np.array(interval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"minimum rescaling of results per interval\"\"\"\n",
    "old_result_handling_min_rescaled =  get_renormalized_log_g_values(walker_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION MIN RESCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(abs(np.concatenate(old_result_handling_min_rescaled[1])- np.array(min_rescaled_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker_results = old_result_handling_out\n",
    "\n",
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(walker_results)\n",
    "\n",
    "results_x = []\n",
    "results_y = []\n",
    "for result in walker_results:\n",
    "    results_y.append(np.array(list(result.values())))\n",
    "    results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Store concatenate interval results\"\"\"\n",
    "concatenated_keys = np.concatenate(results_x)\n",
    "concatenated_values = np.concatenate(results_y)\n",
    "list_of_concat_rescale_dicts = []\n",
    "for keys, values in zip(results_x, results_y):\n",
    "    # Combine keys and values into a dictionary\n",
    "    dict_from_arrays = {k: v for k, v in zip(keys, values)}\n",
    "    list_of_concat_rescale_dicts.append(dict_from_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION CONCATENATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(abs(concatenated_values - np.array(stitched_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"normalize the walker results by min value for log results\"\"\"\n",
    "walker_results = get_renormalized_log_g_values_as_dict_list(walker_results)\n",
    "\n",
    "\"\"\"averages over walker results per intervals\"\"\"\n",
    "walker_results = average_matching_keys(walker_results)\n",
    "\n",
    "results_x = []\n",
    "results_y = []\n",
    "for result in walker_results:\n",
    "    results_y.append(np.array(list(result.values())))\n",
    "    results_x.append(np.array(list(result.keys())))\n",
    "\n",
    "derivatives_wrt_e = get_derivative_wrt_e(walker_results)\n",
    "minimum_deviation_energies = find_lowest_inverse_temp_deviation(derivatives_wrt_e)\n",
    "rescale_results_for_concatenation(results_x, results_y, minimum_deviation_energies)\n",
    "\n",
    "\"\"\"Store concatenate interval results\"\"\"\n",
    "concatenated_keys = np.concatenate(results_x)\n",
    "concatenated_values = np.concatenate(results_y)\n",
    "list_of_concat_rescale_dicts = []\n",
    "for keys, values in zip(results_x, results_y):\n",
    "    # Combine keys and values into a dictionary\n",
    "    dict_from_arrays = {k: v for k, v in zip(keys, values)}\n",
    "    list_of_concat_rescale_dicts.append(dict_from_arrays)\n",
    "\n",
    "\"\"\"Cut overlapping parts\"\"\"\n",
    "x_max = -1 -2*X*Y\n",
    "unique_x = []\n",
    "unique_y = []\n",
    "\n",
    "sorted_dict_list = [dict(sorted(d.items())) for d in list_of_concat_rescale_dicts]\n",
    "\n",
    "for dictionary in sorted_dict_list:\n",
    "    for key, value in dictionary.items():\n",
    "        if key > x_max: #avoid double counting\n",
    "            x_max = key\n",
    "            unique_x.append(key)\n",
    "            unique_y.append(value)\n",
    "\n",
    "offset = log_sum_exp(cut_data_input)\n",
    "rescaled_y = [res + mp.log(2)*16 - offset for res in cut_data_input]\n",
    "print(rescaled_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION CUT OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(unique_y) - np.array(list(cut_data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IMPLIES PROBLEM WITH NAIVE CUT OF OVERLAP IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMING NOW INPUT FOR LOG SUM EXP RESCALING WHICH GOES INTO C CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_data_input = np.array(list(cut_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mpf('2.8611055117440856'), mpf('4.6269746523440851'), mpf('6.6603730898440858'), mpf('8.3006807070440853'), mpf('9.2604707461440849'), mpf('9.7597383242440845'), mpf('9.7613496523440855'), mpf('9.2594209414440858'), mpf('8.2914277773440848'), mpf('6.6883760195440853'), mpf('4.6819551211440853'), mpf('2.9483125429440857')]\n"
     ]
    }
   ],
   "source": [
    "offset = log_sum_exp(cut_data_input)\n",
    "rescaled_y = [res + mp.log(2)*16 - offset for res in cut_data_input]\n",
    "print(rescaled_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION LOG SUM EXP RESCALING FOR SAME INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpf('5.5915272412221384e-11')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(np.array(rescaled_y) - np.array(list(normalized_data.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVE WAY OF CUTTING THE OVERLAP ANALOGOUSLY TO C CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_overlapping_histogram_parts(interval_data, stitching_keys):\n",
    "    for i in range(len(stitching_keys)):\n",
    "        stitching_energy_of_interval_i = stitching_keys[i]\n",
    "\n",
    "        # Modify the i-th interval\n",
    "        current_interval = interval_data[i]\n",
    "        # Keep only keys <= stitching_energy_of_interval_i\n",
    "        current_interval = {k: v for k, v in current_interval.items() if k <= stitching_energy_of_interval_i}\n",
    "\n",
    "        # Modify the (i+1)-th interval if following interval is still in bounds\n",
    "        if i + 1 < len(interval_data):\n",
    "            next_interval = interval_data[i + 1]\n",
    "            # Keep only keys > stitching_energy_of_interval_i\n",
    "            next_interval = {k: v for k, v in next_interval.items() if k > stitching_energy_of_interval_i}\n",
    "\n",
    "        # Update the intervals in the original list\n",
    "        interval_data[i] = current_interval\n",
    "        if i + 1 < len(interval_data):\n",
    "            interval_data[i + 1] = next_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_overlapping_histogram_parts(list_of_concat_rescale_dicts, minimum_deviation_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of dictionaries into a single list of values\n",
    "cut_values = [value for d in list_of_concat_rescale_dicts for value in d.values()]\n",
    "\n",
    "# Convert the list of values into a NumPy array\n",
    "cut_values_array = np.array(cut_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVIATION OF CUT RESULTS NEW IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0010038070950031e-10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(abs(cut_values_array-np.array(list(cut_data.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
